{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c5cc9da-4806-4d72-9fae-0f7ebc57bc94",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-21T15:28:51.107423Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\pinar\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pinar\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Big Data Projekt_Gruppe 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44cfe2f556cba1",
   "metadata": {},
   "source": [
    "### Lebensmittelpreisanalyse: Billa, Hofer & Interspar\n",
    "\n",
    "In diesem Projektabschnitt wurden **Produktdaten aus √∂sterreichischen Superm√§rkten (Billa, Hofer und Interspar)** gesammelt. Fokus lag auf den Kategorien:\n",
    "\n",
    "- Brot & Geb√§ck  \n",
    "- K√ºhlwaren  \n",
    "- Fleisch & Fisch  \n",
    "\n",
    "Mittels **Web Scraping** wurden die Preise und Produktnamen aus den jeweiligen Onlineshops extrahiert und als strukturierte **CSV-Dateien** gespeichert. Diese Dateien dienen als Grundlage f√ºr die sp√§tere Analyse, Visualisierung und Vergleichbarkeit der Preise zwischen den Superm√§rkten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c7119-801e-4344-8de9-d63a0c248c99",
   "metadata": {},
   "source": [
    "## Webscraping bei Interspar mit Selenium\n",
    "\n",
    "In diesem Abschnitt scrapen wir die Produktdaten von **interspar.at** f√ºr drei Kategorien:\n",
    "\n",
    "- Wurst, Fleisch & Fisch\n",
    "- K√ºhlregal\n",
    "- Brot & Geb√§ck\n",
    "\n",
    "### Vorgehensweise:\n",
    "1. Wir rufen jede Kategorie-URL mit **Selenium** im Browser auf.\n",
    "2. Nach dem Laden parsen wir den HTML-Inhalt zus√§tzlich mit **BeautifulSoup**.\n",
    "3. F√ºr jedes Produkt extrahieren wir:\n",
    "   - Den **Produktnamen**\n",
    "   - Den **Preis** (bestehend aus zwei `<label>`-Teilen: Euro und Cent)\n",
    "   - Den **Produktlink**\n",
    "   - Die **Kategorie**, aus der das Produkt stammt\n",
    "4. Wir gehen automatisch durch **alle Seiten** innerhalb der Kategorie (via ‚ÄûN√§chste Seite‚Äú-Button).\n",
    "5. Abschlie√üend speichern wir alle Daten in der Datei **`interspar_kategorien.csv`**.\n",
    "\n",
    "**Ergebnis:**  \n",
    "Eine vollst√§ndige Liste aller Produkte aus den drei Kategorien inkl. Preis und Link, ideal zur sp√§teren Analyse und zum Vergleich mit anderen M√§rkten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df9140457caed5e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-05T13:43:34.268974Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 Produkte erfolgreich gespeichert in 'interspar_kategorien.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Kategorie-URLs\n",
    "urls = [\n",
    "    \"https://www.interspar.at/shop/lebensmittel/wurst-fleisch-fisch/c/F3/\",\n",
    "    \"https://www.interspar.at/shop/lebensmittel/kuehlregal/c/F2/\",\n",
    "    \"https://www.interspar.at/shop/lebensmittel/brot-gebaeck/c/F6/\"\n",
    "]\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "base_url = \"https://www.interspar.at\"\n",
    "product_list = []\n",
    "\n",
    "try:\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        while True:\n",
    "            # Warten auf Produkte\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"productBox\"))\n",
    "            )\n",
    "            time.sleep(1)\n",
    "\n",
    "            # HTML einlesen\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            product_boxes = soup.find_all('div', class_='productBox')\n",
    "\n",
    "            for box in product_boxes:\n",
    "                link_url = box.get('data-url')\n",
    "                link = base_url + link_url if link_url else \"\"\n",
    "\n",
    "                # Preis zusammensetzen aus zwei <label>\n",
    "                try:\n",
    "                    price_container = box.find('div', class_='actualPriceContainer')\n",
    "                    euros = price_container.find('label', class_='priceInteger').text.strip()\n",
    "                    cents = price_container.find('label', class_='priceDecimal').text.strip()\n",
    "                    price = f\"{euros},{cents} ‚Ç¨\"\n",
    "                except:\n",
    "                    price = \"Kein Preis\"\n",
    "\n",
    "                # Produktname (zweiter Titel ist Name)\n",
    "                title_tags = box.find_all('div', class_='productTitle')\n",
    "                if len(title_tags) > 1 and title_tags[1].has_attr('title'):\n",
    "                    product_name = title_tags[1]['title'].strip()\n",
    "                elif title_tags and title_tags[0].has_attr('title'):\n",
    "                    product_name = title_tags[0]['title'].strip()\n",
    "                else:\n",
    "                    product_name = \"Kein Name\"\n",
    "\n",
    "                product_list.append({\n",
    "                    'Kategorie': url.split(\"/\")[-3],  # z.‚ÄØB. kuehlregal\n",
    "                    'Produkt': product_name,\n",
    "                    'Preis': price,\n",
    "                    'Link': link\n",
    "                })\n",
    "\n",
    "            \n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, 'a[title=\"n√§chste Seite\"]')\n",
    "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                    break\n",
    "                next_button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Speichern als CSV\n",
    "with open('interspar_kategorien.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['Kategorie', 'Produkt', 'Preis', 'Link'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"{len(product_list)} Produkte erfolgreich gespeichert in 'interspar_kategorien.csv'\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7558242-77e1-423f-a4f2-201bb4334402",
   "metadata": {},
   "source": [
    "## Webscraping bei Billa ‚Äì mehrere Kategorien mit Selenium\n",
    "\n",
    "Hier nutzen wir **Selenium**, um automatisch Produkte von der Website **shop.billa.at** auszulesen.\n",
    "\n",
    "### Kategorien, die wir analysieren:\n",
    "- K√ºhlwaren\n",
    "- Brot & Geb√§ck\n",
    "\n",
    "### Ablauf:\n",
    "1. Der Browser ruft nacheinander jede URL der Kategorien auf.\n",
    "2. Nach dem vollst√§ndigen Laden der Produktliste (√ºber `WebDriverWait`) werden alle Produktkarten erfasst.\n",
    "3. F√ºr jedes Produkt lesen wir:\n",
    "   - Den **Produktnamen** (aus dem HTML-Attribut `data-teaser-name`)\n",
    "   - Den **Preis**\n",
    "4. Zus√§tzlich speichern wir die jeweilige **Seite/Kategorie**, um die Herkunft nachzuvollziehen.\n",
    "5. Alle Daten werden in der Datei **`billa_mehrere_seiten.csv`** gespeichert.\n",
    "\n",
    "**Ergebnis:**  \n",
    "Eine CSV-Datei mit Produktnamen, Preisen und zugeh√∂riger Kategorie (Seite) ‚Äì bereit f√ºr die weitere Analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Produkte gefunden auf https://shop.billa.at/kategorie/kuehlwaren-13841\n",
      "30 Produkte gefunden auf https://shop.billa.at/kategorie/brot-und-gebaeck-13766\n",
      "60 Produkte gespeichert.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Liste der Billa-Kategorien\n",
    "urls = [\n",
    "    \"https://shop.billa.at/kategorie/kuehlwaren-13841\",\n",
    "    \"https://shop.billa.at/kategorie/brot-und-gebaeck-13766\"\n",
    "]\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "product_list = []\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, \"ws-product-item-base\"))\n",
    "    )\n",
    "    time.sleep(3)\n",
    "\n",
    "    products = driver.find_elements(By.CLASS_NAME, \"ws-product-item-base\")\n",
    "    print(f\"{len(products)} Produkte gefunden auf {url}\")\n",
    "\n",
    "    for product in products:\n",
    "        # Produktname aus Attribut\n",
    "        try:\n",
    "            product_name = product.get_attribute(\"data-teaser-name\")\n",
    "        except:\n",
    "            product_name = \"Kein Name\"\n",
    "\n",
    "        # Preis extrahieren\n",
    "        try:\n",
    "            price = product.find_element(By.CLASS_NAME, \"ws-product-price-type__value\").text.strip()\n",
    "        except:\n",
    "            price = \"Kein Preis\"\n",
    "\n",
    "        product_list.append({'Produkt': product_name, 'Preis': price, 'Seite': url})\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# CSV speichern\n",
    "with open('billa_mehrere_seiten.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['Produkt', 'Preis', 'Seite'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"{len(product_list)} Produkte gespeichert.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d18b4-b052-43c7-bbf9-f37f141d6bab",
   "metadata": {},
   "source": [
    "## Webscraping eines Hofer-Produktbereichs mit Selenium\n",
    "\n",
    "In diesem Schritt wird mithilfe von **Selenium** automatisiert die Produkt√ºbersicht der Kategorie  \n",
    "**‚ÄûFleisch und Fisch‚Äú** auf der Website [hofer.at](https://www.hofer.at) ausgelesen.\n",
    "\n",
    "### Ablauf:\n",
    "- Der **Browser** wird ge√∂ffnet und auf die entsprechende Seite navigiert.\n",
    "- Es wird gewartet, bis alle Produktkarten vollst√§ndig geladen sind.\n",
    "- Dann werden mit `find_elements` alle Produkte in der Liste durchlaufen.\n",
    "- F√ºr jedes Produkt erfassen wir:\n",
    "  - den **Produktnamen**\n",
    "  - den **Preis**\n",
    "- Die gesammelten Informationen werden in eine **Liste von Dictionaries** geschrieben.\n",
    "- Anschlie√üend speichern wir alle Eintr√§ge in einer **CSV-Datei namens `hofer_produkte.csv`**.\n",
    "\n",
    "Ergebnis:  \n",
    "Die Datei enth√§lt eine strukturierte Liste mit Produktnamen und Preisen aus der Fleisch-und-Fisch-Kategorie von Hofer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65e58a1b7e04493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Produkte: 100\n",
      "100 Produkte gespeichert in 'hofer_produkte.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Starte Browser\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.hofer.at/de/sortiment/produktsortiment/fleisch-und-fisch.html\")\n",
    "\n",
    "# Warte bis Produktkarten sichtbar sind\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"item.plp_product\"))\n",
    ")\n",
    "time.sleep(2)\n",
    "\n",
    "# Finde alle Produkte\n",
    "products = driver.find_elements(By.CLASS_NAME, \"item.plp_product\")\n",
    "print(f\"Gefundene Produkte: {len(products)}\")\n",
    "\n",
    "product_list = []\n",
    "\n",
    "for product in products:\n",
    "    # Produktname aus h2\n",
    "    try:\n",
    "        name_tag = product.find_element(By.CLASS_NAME, \"product-title\")\n",
    "        product_name = name_tag.text.strip()\n",
    "    except:\n",
    "        product_name = \"Kein Name\"\n",
    "\n",
    "    # Preis aus span\n",
    "    try:\n",
    "        price_tag = product.find_element(By.CLASS_NAME, \"at-product-price_lbl\")\n",
    "        price = price_tag.text.strip()\n",
    "    except:\n",
    "        price = \"Kein Preis\"\n",
    "\n",
    "    product_list.append({'Produkt': product_name, 'Preis': price})\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Export als CSV\n",
    "with open('hofer_produkte.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['Produkt', 'Preis'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"{len(product_list)} Produkte gespeichert in 'hofer_produkte.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20630ec0-bc3c-4067-bb97-27ffd0880d8a",
   "metadata": {},
   "source": [
    "## Webscraping mit Selenium ‚Äì Hofer\n",
    "\n",
    "Mit diesem Skript sammeln wir Produktinformationen von der Website **hofer.at**.  \n",
    "Es werden folgende Produktkategorien automatisiert durchsucht:\n",
    "\n",
    "- Fleisch und Fisch\n",
    "- K√ºhlwaren\n",
    "- Vorratsschrank\n",
    "\n",
    "F√ºr jedes Produkt speichern wir:\n",
    "- **Kategorie**\n",
    "- **Produktname**\n",
    "- **Preis**\n",
    " \n",
    "Die gesammelten Daten werden am Ende in der Datei `hofer_kategorien.csv` gespeichert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f8e8d-633c-44ff-aaf1-8a3758f23d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Kategorien-URLs\n",
    "urls = [\n",
    "    \"https://www.hofer.at/de/sortiment/produktsortiment/fleisch-und-fisch.html\",\n",
    "    \"https://www.hofer.at/de/sortiment/produktsortiment/kuehlung.html\",\n",
    "    \"https://www.hofer.at/de/sortiment/produktsortiment/vorratsschrank.html\"\n",
    "]\n",
    "\n",
    "# Starte Browser\n",
    "driver = webdriver.Chrome()\n",
    "product_list = []\n",
    "\n",
    "try:\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Warte auf Produkte\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.CLASS_NAME, \"item.plp_product\"))\n",
    "                )\n",
    "                time.sleep(1)\n",
    "\n",
    "                products = driver.find_elements(By.CLASS_NAME, \"item.plp_product\")\n",
    "                print(f\"{len(products)} Produkte auf Seite in Kategorie: {url.split('/')[-1]}\")\n",
    "\n",
    "                for product in products:\n",
    "                    # Produktname\n",
    "                    try:\n",
    "                        name_tag = product.find_element(By.CLASS_NAME, \"product-title\")\n",
    "                        product_name = name_tag.text.strip()\n",
    "                    except:\n",
    "                        product_name = \"Kein Name\"\n",
    "\n",
    "                    # Preis\n",
    "                    try:\n",
    "                        price_tag = product.find_element(By.CLASS_NAME, \"at-product-price_lbl\")\n",
    "                        price = price_tag.text.strip()\n",
    "                    except:\n",
    "                        price = \"Kein Preis\"\n",
    "\n",
    "                    # Zur Liste hinzuf√ºgen\n",
    "                    product_list.append({\n",
    "                        'Kategorie': url.split('/')[-1].replace('.html', ''),\n",
    "                        'Produkt': product_name,\n",
    "                        'Preis': price\n",
    "                    })\n",
    "\n",
    "                # Weiter-Button pr√ºfen\n",
    "                next_button = driver.find_element(By.CLASS_NAME, \"pagination-next\")\n",
    "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                    break\n",
    "                else:\n",
    "                    next_button.click()\n",
    "                    time.sleep(2)\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Speichere alles als CSV\n",
    "with open('hofer_kategorien.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['Kategorie', 'Produkt', 'Preis'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(product_list)\n",
    "\n",
    "print(f\"{len(product_list)} Produkte aus allen Kategorien gespeichert in 'hofer_kategorien.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3995f8-c63c-4dff-89b6-4dcd80bfd4c5",
   "metadata": {},
   "source": [
    "## üîó Verbindung zu MongoDB & Datenimport\n",
    "\n",
    "In diesem Schritt verbinden wir uns mit der lokalen MongoDB-Datenbank und importieren CSV-Dateien mit Produktdaten aus drei Superm√§rkten:\n",
    "\n",
    "- **Billa**\n",
    "- **Hofer**\n",
    "- **Interspar**\n",
    "\n",
    "Jede Datei wird als DataFrame geladen, mit dem Namen des Supermarkts versehen und anschlie√üend als JSON-√§hnliche Dokumente in die MongoDB-Collection `lebensmittel` geschrieben.\n",
    "\n",
    "Damit haben wir eine zentrale, NoSQL-basierte Datenspeicherung, auf die wir im weiteren Verlauf flexibel zugreifen k√∂nnen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b887911707a010e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:52:26.512592Z",
     "start_time": "2025-04-19T16:52:12.938486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.12/site-packages (4.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b1e7de98c3b84e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Datens√§tze aus 'Billa' importiert.\n",
      "300 Datens√§tze aus 'Hofer' importiert.\n",
      "240 Datens√§tze aus 'Interspar' importiert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Verbindung zur MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"supermarkt_preise\"]\n",
    "collection = db[\"lebensmittel\"]\n",
    "\n",
    "# Optional: Bestehende Daten vorher l√∂schen\n",
    "collection.delete_many({})\n",
    "\n",
    "# Deine Dateien + zugeh√∂riger Supermarktname\n",
    "files_with_market = {\n",
    "    \"../data/billa_mehrere_seiten.csv\": \"Billa\",\n",
    "    \"../data/hofer_kategorien.csv\": \"Hofer\",\n",
    "    \"../data/interspar_kategorien.csv\": \"Interspar\"\n",
    "}\n",
    "\n",
    "# Alle in dieselbe Collection importieren\n",
    "for filepath, supermarkt in files_with_market.items():\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        df[\"Supermarkt\"] = supermarkt  \n",
    "        data = df.to_dict(orient=\"records\")\n",
    "        collection.insert_many(data)\n",
    "        print(f\"{len(data)} Datens√§tze aus '{supermarkt}' importiert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Datei {filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d7e5d-c15e-4f66-9b7f-b32c8a994d77",
   "metadata": {},
   "source": [
    "## Fazit ‚Äì Datensammlung per Webscraping\n",
    "\n",
    "Wir haben erfolgreich Produktdaten aus drei gro√üen √∂sterreichischen Superm√§rkten gesammelt:\n",
    "\n",
    "- **Billa** (√ºber Kategorie-URLs)\n",
    "- **Hofer** (inkl. Navigation durch Seiten)\n",
    "- **Interspar** (kombiniert Selenium + BeautifulSoup)\n",
    "\n",
    "Dabei wurden folgende Informationen extrahiert und gespeichert:\n",
    "- Produktname\n",
    "- Preis\n",
    "- Kategorie\n",
    "- Link (Interspar)\n",
    "\n",
    "Die gesammelten Daten wurden als **CSV-Dateien** exportiert und stehen nun f√ºr weitere Analysen bereit ‚Äì z.‚ÄØB. Preisvergleiche oder MapReduce-Berechnungen.\n",
    "\n",
    "**Ergebnis:**  \n",
    "Wir verf√ºgen nun √ºber eine konsistente und strukturierte Datengrundlage aus mehreren Quellen, die sich f√ºr Big-Data-Analysen eignet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
